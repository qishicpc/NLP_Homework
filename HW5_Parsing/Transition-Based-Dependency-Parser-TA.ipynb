{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this homework you will implement the transition based depensency parser for projective tree.\n",
    "\n",
    "Take a look at the training file. It has the [CoNLL format](http://ufal.mff.cuni.cz/conll2009-st/task-description.html) format, including the following columns:\n",
    "\n",
    "| ID | TOKEN | LEMMA | UNIV_POS | TREEBANK_POS | FEAT | HEAD | DEP_RELATION |\n",
    "|----|-------|-------|----------|--------------|------|------|--------------|\n",
    "| 1  | This | this | DET | DT |  _ | 0 | det|\n",
    "| 2  | ... | ... | ... | ... |  ... | ... | ... |\n",
    "\n",
    "**ID:**           (int) Index of a word in a sentence, starting from 1 <br>\n",
    "**TOKEN**:        (str) The word itself <br>\n",
    "**TREEBANK_POS**: (str) The POS tag of the word <br>\n",
    "**HEAD**:         (int) Head of the dependent <br>\n",
    "**DEP_RELATION**: (str) The label of the dependency relationship \n",
    "\n",
    "In this homework, we will not need the UNIV_POS and FEAT columns.\n",
    "\n",
    "To help you with the homework, we provide the sanity_check.py script. However the script depends on the function you implement in this notebook, so it may not work by directly running in the console. You can copy the whole file content into the `Unit test` session, and run the check functions as you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T05:31:20.462505Z",
     "start_time": "2019-04-12T05:31:19.823633Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys,re\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T06:24:57.309420Z",
     "start_time": "2019-04-12T06:24:57.265042Z"
    }
   },
   "outputs": [],
   "source": [
    "MINIMUM_FEAT_COUNT=5\n",
    "\n",
    "# dictionary of feature: index\n",
    "# feature = a dictionary of feature name: value\n",
    "feature_vocab={}\n",
    "\n",
    "# reverse_features[index] = feature\n",
    "reverse_features=[]\n",
    "\n",
    "# dictionary of label: index, where\n",
    "# label = SHIFT, LEFTARC_DEPENDENCY_LABEL, RIGHTARC_DEPENDENCY_LABEL\n",
    "label_vocab={}\n",
    "\n",
    "# reverse_labels[index] = label\n",
    "reverse_labels=[]\n",
    "\n",
    "# number/ID of features\n",
    "fmax = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Projectivity \n",
    "\n",
    "The transition based dependency parsing algorithm features linear time complexity, but it works only for projective tree. So the first step we need is to check if a tree is projective. If you need help, feel free to google for working solutions. Use the `check_projective` function from the sanity_check.py file to check your projective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T06:29:58.542600Z",
     "start_time": "2019-04-12T06:29:58.505290Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_projective(toks):\n",
    "    \"\"\"\n",
    "    params: toks is a list of (idd, tok, pos, head, lab) for a sentence\n",
    "    return True if and only if the sentence has a projective dependency tree\n",
    "    \"\"\"\n",
    "    # Implement your code below\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Oracle\n",
    "\n",
    "The training data in the conll file contains the nodes and edges of the parsing tree: \n",
    "* The initial **wbuffer** is the nodes in the tree\n",
    "* The deps is the edges in the tree wich structure like: <br>\n",
    "    deps = {head1:{\n",
    "                  (head1, child1):dependency_label1,\n",
    "                  (head1, child2):dependency_label2\n",
    "                 }\n",
    "            head2:{\n",
    "                  (head2, child3):dependency_label3,\n",
    "                  (head2, child4):dependency_label4\n",
    "                 }\n",
    "            }\n",
    "We need to translate each tree into a list of configurations and gold_transitions (label), so later on we can featurize configuration and apply machine learning models on it.\n",
    "\n",
    "Rember to use the `sanity_check` to help you with the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T06:11:29.511348Z",
     "start_time": "2019-04-12T06:11:29.468154Z"
    }
   },
   "outputs": [],
   "source": [
    "def perform_shift(wbuffer, stack, arcs,\n",
    "                  configurations, gold_transitions):\n",
    "    \"\"\"\n",
    "    perform the SHIFT operation\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement your code below\n",
    "    # your code should:\n",
    "    # 1. append the latest configuration to configurations\n",
    "    # 2. append the latest action to gold_transitions\n",
    "    # 3. update wbuffer, stack and arcs accordingly\n",
    "    # hint: note that the order of operations matters\n",
    "    # as we want to capture the configurations and transition rules\n",
    "    # before making changes to the stack, wbuffer and arcs\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def perform_arc(direction, dep_label,\n",
    "                wbuffer, stack, arcs,\n",
    "                configurations, gold_transitions):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        - direction: {\"LEFT\", \"RIGHT\"}\n",
    "        - dep_label: label for the dependency relations\n",
    "    Perform LEFTARC_ and RIGHTARC_ operations\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement your code below\n",
    "    # your code should:\n",
    "    # 1. append the latest configuration to configurations\n",
    "    # 2. append the latest action to gold_transitions\n",
    "    # 3. update wbuffer, stack and arcs accordingly\n",
    "    # hint: note that the order of operations matters\n",
    "    # as we want to capture the configurations and transition rules\n",
    "    # before making changes to the stack, wbuffer and arcs\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def tree_to_actions(wbuffer, stack, arcs, deps):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    wbuffer: a list of word indices, top of buffer is at the end of the list\n",
    "    stack: a list of word indices, top of buffer is at the end of the list\n",
    "    arcs: a list of (label, head, dependent) tuples\n",
    "\n",
    "    Given wbuffer, stack, arcs and deps\n",
    "    Return configurations and gold_transitions (actions)\n",
    "    \"\"\"\n",
    "\n",
    "    # configurations:\n",
    "    # A list of tuples of lists\n",
    "    # [(wbuffer1, stack1, arcs1), (wbuffer2, stack2, arcs2), ...]\n",
    "    # Keeps tracks of the states at each step\n",
    "    configurations=[]\n",
    "\n",
    "    # gold_transitions:\n",
    "    # A list of action strings, e.g [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
    "    # Keeps tracks of the actions at each step\n",
    "    gold_transitions=[]\n",
    "\n",
    "    # Implement your code below\n",
    "    # hint:\n",
    "    # 1. configurations[i] and gold_transitions[i] should\n",
    "    # correspond to the states of the wbuffer, stack, arcs\n",
    "    # (before the action was take) and action to take at step i\n",
    "    # 2. you should call perform_shift and perform_arc in your code\n",
    "    \n",
    "    raise NotImplementedError\n",
    "            \n",
    "    return configurations, gold_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Parsing with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T05:31:33.328951Z",
     "start_time": "2019-04-12T05:31:33.286543Z"
    }
   },
   "outputs": [],
   "source": [
    "def isvalid(stack, wbuffer, action):\n",
    "    \"\"\"\n",
    "    Helper function that returns True only if an action is\n",
    "    legal given the current states of the stack and wbuffer\n",
    "    \"\"\"\n",
    "    if action == \"SHIFT\" and len(wbuffer) > 0:\n",
    "        return True\n",
    "    if action.startswith(\"RIGHTARC\") and len(stack) > 1 and stack[-1] != 0:\n",
    "        return True\n",
    "    if action.startswith(\"LEFTARC\") and len(stack) > 1 and stack[-2] != 0:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def action_to_tree(tree, predictions, wbuffer, stack, arcs):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    tree:\n",
    "    a dictionary of dependency relations (head, dep_label)\n",
    "        {\n",
    "            child1: (head1, dep_lebel1),\n",
    "            child2: (head2, dep_label2), ...\n",
    "        }\n",
    "\n",
    "    predictions:\n",
    "    a numpy column vector of probabilities for different dependency labels\n",
    "    as ordered by the global variable reverse_labels\n",
    "    predictions.shape = (1, total number of dependency labels)\n",
    "\n",
    "    wbuffer: a list of word indices, top of buffer is at the end of the list\n",
    "    stack: a list of word indices, top of buffer is at the end of the list\n",
    "    arcs: a list of (label, head, dependent) tuples\n",
    "\n",
    "    \"\"\"\n",
    "    global reverse_labels\n",
    "\n",
    "    # Implement your code below\n",
    "    # hint:\n",
    "    # 1. the predictions contains the probability distribution for all\n",
    "    # possible actions for a single step, and you should choose one\n",
    "    # and update the tree only once\n",
    "    # 2. some actions predicted are not going to be valid\n",
    "    # (e.g., shifting if nothing is on the buffer)\n",
    "    # so sort probs and keep going until we find one that is valid.\n",
    "    raise NotImplementedError\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get_oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T05:51:14.691090Z",
     "start_time": "2019-04-12T05:51:14.637336Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# THE FOLLOWING CODE IS PROVIDED\n",
    "# ============================================================\n",
    "def get_oracle(toks):\n",
    "    \"\"\"\n",
    "    Return pairs of configurations + gold transitions (actions)\n",
    "    from training data\n",
    "    configuration = a list of tuple of:\n",
    "        - buffer (top of buffer is at the end of the list)\n",
    "        - stack (top of buffer is at the end of the list)\n",
    "        - arcs (a list of (label, head, dependent) tuples)\n",
    "    gold transitions = a list of actions, e.g. SHIFT\n",
    "    \"\"\"\n",
    "\n",
    "    stack = [] # stack\n",
    "    arcs = [] # existing list of arcs\n",
    "    wbuffer = [] # input buffer\n",
    "\n",
    "    # deps is a dictionary of head: dependency relations, where\n",
    "    # dependency relations is a dictionary of the (head, child): label\n",
    "    # deps = {head1:{\n",
    "    #               (head1, child1):dependency_label1,\n",
    "    #               (head1, child2):dependency_label2\n",
    "    #              }\n",
    "    #         head2:{\n",
    "    #               (head2, child3):dependency_label3,\n",
    "    #               (head2, child4):dependency_label4\n",
    "    #              }\n",
    "    #         }\n",
    "    deps = {}\n",
    "\n",
    "    # ROOT\n",
    "    stack.append(0)\n",
    "\n",
    "    # initialize variables\n",
    "    for position in reversed(toks):\n",
    "        (idd, _, _, head, lab) = position\n",
    "\n",
    "        dep = (head, idd)\n",
    "        if head not in deps:\n",
    "            deps[head] = {}\n",
    "        deps[head][dep] = lab\n",
    "\n",
    "        wbuffer.append(idd)\n",
    "\n",
    "    # configurations:\n",
    "    # A list of (wbuffer, stack, arcs)\n",
    "    # Keeps tracks of the states at each step\n",
    "    # gold_transitions:\n",
    "    # A list of action strings [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
    "    # Keeps tracks of the actions at each step\n",
    "    configurations, gold_transitions = tree_to_actions(wbuffer, stack, arcs, deps)\n",
    "    return configurations, gold_transitions\n",
    "\n",
    "\n",
    "# Bonus Question\n",
    "def featurize_configuration(configuration, tokens, postags):\n",
    "    \"\"\"\n",
    "    !EXTRA CREDIT!:\n",
    "    Add new features here to improve the performance of the parser\n",
    "\n",
    "    Given configurations of the stack, input buffer and arcs,\n",
    "    words of the sentence and POS tags of the words,\n",
    "    return some features\n",
    "    \"\"\"\n",
    "    wbuffer, stack, arcs = configuration\n",
    "    feats = {}\n",
    "\n",
    "    feats[\"%s_%s\" % (\"len_buffer\", len(wbuffer))] = 1\n",
    "    feats[\"%s_%s\" % (\"len_stack\", len(stack))] = 1\n",
    "\n",
    "    # single-word features\n",
    "    if len(stack) > 0:\n",
    "        feats[\"%s_%s\" % (\"stack_word_1\", tokens[stack[-1]])] = 1\n",
    "        feats[\"%s_%s\" % (\"stack_tag_1\", postags[stack[-1]])] = 1\n",
    "        feats[\"%s_%s_%s\" % (\"stack_tag_word_1\", tokens[stack[-1]], postags[stack[-1]])] = 1\n",
    "\n",
    "    if len(stack) > 1:\n",
    "        feats[\"%s_%s\" % (\"stack_word_2\", tokens[stack[-2]])] = 1\n",
    "        feats[\"%s_%s\" % (\"stack_tag_2\", postags[stack[-2]])] = 1\n",
    "        feats[\"%s_%s_%s\" % (\"stack_tag_word_2\", tokens[stack[-2]], postags[stack[-2]])] = 1\n",
    "\n",
    "    if len(wbuffer) > 0:\n",
    "        feats[\"%s_%s\" % (\"buffer_word_1\", tokens[wbuffer[-1]])] = 1\n",
    "        feats[\"%s_%s\" % (\"buffer_tag_1\", postags[wbuffer[-1]])] = 1\n",
    "        feats[\"%s_%s_%s\" % (\"buffer_tag_word_1\", tokens[wbuffer[-1]], postags[wbuffer[-1]])] = 1\n",
    "\n",
    "    # word-pair features\n",
    "    if len(stack) > 1:\n",
    "        feats[\"%s_%s_%s_%s\" % (\"stack1_word_tag_stack2_tag\", tokens[stack[-1]], postags[stack[-1]], postags[stack[-2]])] = 1\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def get_oracles(filename):\n",
    "    \"\"\"\n",
    "    Get configurations, gold_transitions from all sentences\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        toks, tokens, postags = [], {}, {}\n",
    "        tokens[0] = \"ROOT\"\n",
    "        postags[0] = \"ROOT\"\n",
    "\n",
    "        # a list of all features for each transition step\n",
    "        feats = []\n",
    "        # a list of labels, e.g. SHIFT, LEFTARC_DEP_LABEL, RIGHTARC_DEP_LABEL\n",
    "        labels = []\n",
    "\n",
    "        for line in f:\n",
    "            cols = line.rstrip().split(\"\\t\")\n",
    "\n",
    "            if len(cols) < 2: # at the end of each sentence\n",
    "                if len(toks) > 0:\n",
    "                    if is_projective(toks): # only use projective trees\n",
    "                        # get all configurations and gold standard transitions\n",
    "                        configurations, gold_transitions = get_oracle(toks)\n",
    "\n",
    "                        for i in range(len(configurations)):\n",
    "                            feat = featurize_configuration(configurations[i], tokens, postags)\n",
    "                            label = gold_transitions[i]\n",
    "                            feats.append(feat)\n",
    "                            labels.append(label)\n",
    "\n",
    "                    # reset vars for the next sentence\n",
    "                    toks, tokens, postags = [], {}, {}\n",
    "                    tokens[0] = \"ROOT\"\n",
    "                    postags[0] = \"ROOT\"\n",
    "                continue\n",
    "\n",
    "            if cols[0].startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # construct the tuple for each word in the sentence\n",
    "            # for each word in the sentence\n",
    "            # idd: index of a word in a sentence, starting from 1\n",
    "            # tok: the word itself\n",
    "            # pos: pos tag for that word\n",
    "            # head: parent of the dependency\n",
    "            # lab: dependency relation label\n",
    "            idd, tok, pos, head, lab = int(cols[0]), cols[1], cols[4], int(cols[6]), cols[7]\n",
    "            toks.append((idd, tok, pos, head, lab))\n",
    "\n",
    "            # feature for training to predict the gold transition\n",
    "            tokens[idd], postags[idd] = tok, pos\n",
    "\n",
    "        return feats, labels\n",
    "\n",
    "\n",
    "def train(feats, labels):\n",
    "    \"\"\"\n",
    "    Train transition-based parsed to predict next action (labels)\n",
    "    given current configuration (featurized by feats)\n",
    "    Return the classifier trained using the logistic regression model\n",
    "    \"\"\"\n",
    "    global feature_vocab, label_vocab, fmax, reverse_labels, reverse_features\n",
    "\n",
    "    lid = 0 # label ID\n",
    "    D = len(feats) # each row of feats corresponds to a row in labels\n",
    "    feature_counts = Counter()\n",
    "\n",
    "    # build dictionary of labels\n",
    "    for i in range(D):\n",
    "        for f in feats[i]:\n",
    "            feature_counts[f] += 1\n",
    "\n",
    "        if labels[i] not in label_vocab:\n",
    "            label_vocab[labels[i]] = lid\n",
    "            lid += 1\n",
    "\n",
    "    # build dictionary of features\n",
    "    for f in feature_counts:\n",
    "        if feature_counts[f] > MINIMUM_FEAT_COUNT and f not in feature_vocab:\n",
    "            feature_vocab[f] = fmax\n",
    "            fmax += 1\n",
    "\n",
    "    # build reverse lookup for features and labels\n",
    "    reverse_labels = [None]*lid\n",
    "    for label in label_vocab:\n",
    "        reverse_labels[label_vocab[label]] = label\n",
    "\n",
    "    reverse_features = [None]*fmax\n",
    "    for feature in feature_vocab:\n",
    "        reverse_features[feature_vocab[feature]] = feature\n",
    "\n",
    "    # X is a D-by-fmax matrix, where each row represents\n",
    "    # features for a configuration / actions\n",
    "    # Y is a list of labels for all configurations\n",
    "    X = sparse.lil_matrix((D, fmax))\n",
    "    Y = []\n",
    "    for i in range(D):\n",
    "        for f in feats[i]:\n",
    "            if f in feature_vocab:\n",
    "                fid = feature_vocab[f]\n",
    "                X[i,fid] = 1\n",
    "        Y.append(label_vocab[labels[i]])\n",
    "\n",
    "    print (\"Docs: \", D, \"Features: \", fmax)\n",
    "    log_reg = (linear_model.LogisticRegression(C=1.0, penalty='l2', multi_class=\"auto\")\n",
    "                           .fit(sparse.coo_matrix(X), Y))\n",
    "\n",
    "    return log_reg\n",
    "\n",
    "\n",
    "def parse(toks, logreg):\n",
    "    \"\"\"\n",
    "    parse sentence with trained model and return correctness measure\n",
    "    \"\"\"\n",
    "    tokens, postags = {}, {}\n",
    "    tokens[0] = \"ROOT\"\n",
    "    postags[0] = \"ROOT\"\n",
    "\n",
    "    heads, labels = {}, {}\n",
    "\n",
    "    wbuffer, stack, arcs = [], [], []\n",
    "    stack.append(0)\n",
    "\n",
    "    for position in reversed(toks):\n",
    "        # featurization\n",
    "        (idd, tok, pos, head, lab) = position\n",
    "        tokens[idd] = tok\n",
    "        postags[idd] = pos\n",
    "\n",
    "        # keep track of gold standards for performance evaluation\n",
    "        heads[idd], labels[idd] = head, lab\n",
    "\n",
    "        # update buffer\n",
    "        wbuffer.append(idd)\n",
    "\n",
    "    tree = {}\n",
    "    while len(wbuffer) >= 0:\n",
    "        if len(wbuffer) == 0 and len(stack) == 0: break\n",
    "        if len(wbuffer) == 0 and len(stack) == 1 and stack[0] == 0: break\n",
    "\n",
    "        feats = (featurize_configuration((wbuffer, stack, arcs), tokens, postags))\n",
    "        X = sparse.lil_matrix((1, fmax))\n",
    "        for f in feats:\n",
    "            if f in feature_vocab:\n",
    "                X[0,feature_vocab[f]]=1\n",
    "\n",
    "        predictions = logreg.predict_proba(X)\n",
    "        # your function will be called here\n",
    "        action_to_tree(tree, predictions, wbuffer, stack, arcs)\n",
    "\n",
    "    # correct_unlabeled: total number of correct (head, child) dependencies\n",
    "    # correct_labeled: total number of correctly *labeled* dependencies\n",
    "    correct_unlabeled, correct_labeled, total = 0, 0, 0\n",
    "\n",
    "    for child in tree:\n",
    "        (head, label) = tree[child]\n",
    "        if head == heads[child]:\n",
    "            correct_unlabeled += 1\n",
    "            if label == labels[child]: correct_labeled += 1\n",
    "        total += 1\n",
    "\n",
    "    return [correct_unlabeled, correct_labeled, total]\n",
    "\n",
    "\n",
    "def evaluate(filename, logreg):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a parser against gold standard\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        toks=[]\n",
    "        totals = np.zeros(3)\n",
    "        for line in f:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "\n",
    "            if len(cols) < 2: # end of a sentence\n",
    "                if len(toks) > 0:\n",
    "                    if is_projective(toks):\n",
    "                        tots = np.array(parse(toks, logreg))\n",
    "                        totals += tots\n",
    "                        print (\"%.3f\\t%.3f\\t%s\" % (totals[0]/totals[2], totals[1]/totals[2], totals))\n",
    "                    toks = []\n",
    "                continue\n",
    "\n",
    "            if cols[0].startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            idd, tok, pos, head, lab = int(cols[0]), cols[1], cols[4], int(cols[6]), cols[7]\n",
    "            toks.append((idd, tok, pos, head, lab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T06:32:01.270819Z",
     "start_time": "2019-04-12T06:32:01.093976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_oracles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bfc06c5becb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# # ============================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oracles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.projective.short.conll\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# feats, labels = get_oracles(\"train.projective.conll\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_oracles' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Run this cell to train the model\n",
    "#\n",
    "# NOTE: this takes a while to train, so you can change \n",
    "# the input to \"train.projective.short.conll\" to \n",
    "# train on only a subset of the data\n",
    "#\n",
    "# # ============================================================\n",
    "feats, labels = get_oracles(\"train.projective.short.conll\")\n",
    "# feats, labels = get_oracles(\"train.projective.conll\")\n",
    "logreg = train(feats, labels)\n",
    "evaluate(\"dev.projective.conll\", logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
