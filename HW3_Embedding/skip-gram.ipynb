{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework due data: 04/05/2019 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this homework you will be given a chance to explore the properties of word embedding using a pre-trained embedding. Then you will build and train your own embedding using the skip-gram method.\n",
    "\n",
    "Here are some reading materials:\n",
    "\n",
    "1. [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)\n",
    "\n",
    "Recent Turing Reward winner Geoffrey Hinton and coworkers first introduced the concept of words embedding in their 1986 Nature paper.\n",
    "\n",
    "2. [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "Google's word2vec project built on skip-gram and google news data.\n",
    "\n",
    "3. [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "   [Distributed Representations of Words and Phrases\n",
    "and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "\n",
    "Tomas Mikolov from Google published these two papers in 2013 proposing the skip-gram approach for word embedding which has become one of the most popular word embedding.\n",
    "\n",
    "4. [On word embeddings](http://ruder.io/word-embeddings-1/index.html)\n",
    "\n",
    "An online blog by DeepMind engineer Sebastian Ruder explaining skip-gram. I found it easier to understand than the original papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T00:37:17.444332Z",
     "start_time": "2019-03-30T00:37:17.356329Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn, keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with pretrained embedding\n",
    "\n",
    "Before we start training our own words embedding, let's play with pretrained embeddings, so you know what you can expect from your own models. Here we use a very popular embedding called [GloVe](https://nlp.stanford.edu/projects/glove/) developed by standford university. The method used to produce this embedding is based on the factorization of word-word similarity matrix. Worth to notice, thi method is quite different to the skip-gram method we are going to implement later.\n",
    "\n",
    "First let's load the embedding as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:06:50.536096Z",
     "start_time": "2019-03-30T01:06:33.233442Z"
    }
   },
   "outputs": [],
   "source": [
    "glove = pd.read_csv(\"glove_6B_100d_top100k.csv\"); glove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest words\n",
    "One of the many motivations that people are interested in words embedding is that it reveals similarities between words. Let's first check how this works with GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:00:54.394796Z",
     "start_time": "2019-03-30T01:00:54.329543Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "def find_nearest(embedding, word=None, n=5, distance=euclidean_distances):\n",
    "    \"\"\"\n",
    "    For given embedding matrix and a given word, find the n nearest words in the embedding space\n",
    "    \n",
    "    input:\n",
    "        embedding: DataFrame, look at `glove` \n",
    "        word: string, must be in the index of embedding dataframe\n",
    "        n: int, number of nearest words\n",
    "        distance: fucntion, it should at least support the euclidean_distances and cosine_distances\n",
    "        \n",
    "    return:\n",
    "        A series with word as index, distance as value, sorted from lower to high\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Write your code here\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:03:37.284629Z",
     "start_time": "2019-03-30T01:03:37.132508Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Using euclidean_distances, the closest words to frog are:\")\n",
    "print(find_nearest(glove, 'lion'))\n",
    "print(\"Using cosine_distances, the closest words to frog are:\")\n",
    "print(find_nearest(glove, 'lion', distance=cosine_distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:05:13.828526Z",
     "start_time": "2019-03-29T02:05:13.732489Z"
    }
   },
   "source": [
    "What have you observed? Does the result make sense to you? Play with some other words, and see if you can find something interesting. Try countries and numebrs :). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest words with vector\n",
    "Remember that at the beginning of the course we advertised the ability of word embedding being able to find relative relationship between words, such as king - male + female = queen. Let's test this with the embedding we have. But before that we need a method that's similar to find_nearest, but instead of taking a word, it takes an embedding vector as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:00:56.837246Z",
     "start_time": "2019-03-30T01:00:56.773061Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_nearest_with_vector(embedding, vector=None, n=5, distance=euclidean_distances):\n",
    "    \"\"\"\n",
    "    For given embedding matrix and a given vector, find the n nearest words in the embedding space\n",
    "    \n",
    "    input:\n",
    "        embedding: DataFrame, look at `glove` \n",
    "        vector: Series, looks like a coloumn vector of the embedding dataframe\n",
    "        n: int, number of nearest words\n",
    "        distance: fucntion, it should at least support the euclidean_distances and cosine_distances\n",
    "        \n",
    "    return:\n",
    "        A series with word as index, distance as value, sorted from lower to high\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Write your code here\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T06:02:47.514735Z",
     "start_time": "2019-03-29T06:02:47.422576Z"
    }
   },
   "outputs": [],
   "source": [
    "find_nearest_with_vector(glove, glove['king']-glove['male']+glove['female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T06:26:16.475783Z",
     "start_time": "2019-03-29T06:26:16.401102Z"
    }
   },
   "outputs": [],
   "source": [
    "find_nearest_with_vector(glove, glove['china']+glove['capital'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T06:04:08.834136Z",
     "start_time": "2019-03-29T06:04:08.755134Z"
    }
   },
   "source": [
    "What did you see? Can you explore some other interesting relations? Like countries vs cities, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clustering\n",
    "\n",
    "Another feature of the word embedding is that it can cluster similar word in to the same cluster while keep semantic relationship with other clusters. Try the following dimention reduction code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T00:46:27.330743Z",
     "start_time": "2019-03-30T00:46:27.256371Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_2D(X, labels):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = 0.1 + 0.8 * (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for x, lab in zip(X, labels):\n",
    "        plt.text(x[0], x[1], str(lab), fontdict={'size': 14})\n",
    "        \n",
    "def plot_words_embedding(embedding, words):\n",
    "    X = PCA(n_components=2).fit_transform(embedding[words].transpose())\n",
    "    plot_2D(X, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:31:43.931477Z",
     "start_time": "2019-03-30T01:31:43.378663Z"
    }
   },
   "outputs": [],
   "source": [
    "words = ['china', 'beijing', 'russia', 'moscow', 'poland', 'warsaw', 'japan', 'tokyo',\n",
    "        'france', 'paris', 'germany', 'berlin', 'italy', 'rome', 'spain', 'madrid']\n",
    "\n",
    "plot_words_embedding(glove, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you spot something interesting? Try with some other words set and see what you can find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:09:44.430365Z",
     "start_time": "2019-03-30T01:09:38.857615Z"
    }
   },
   "outputs": [],
   "source": [
    "from tools import load_data, show_model\n",
    "\n",
    "text = load_data()\n",
    "\n",
    "print(\"Number of summarys: \", len(text))\n",
    "print(\"Number of words:\", len([w for s in text for w in s]))\n",
    "print(\"Vocabulary size:\", len({w for s in text for w in s}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T04:00:19.581151Z",
     "start_time": "2019-03-28T04:00:19.515978Z"
    }
   },
   "source": [
    "There are about 200K unique words in this corpus. To make it more computational feasible, let's reduce the size of the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:09:55.647335Z",
     "start_time": "2019-03-30T01:09:53.712323Z"
    }
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 20\n",
    "def create_encoder(text, min_count=20):\n",
    "    \"\"\"\n",
    "    - Create a encoder which is a dictionary like {word: index}\n",
    "    - To reduce the total number of vocabularies, you can remove \n",
    "    the words that appear for less than min_count times in the entire\n",
    "    corpus\n",
    "    - Enfore {'_unknown_': 0}\n",
    "    \n",
    "    input:\n",
    "        text: list of token list, e.g. [['i', 'am', 'fine'], ['another', 'summary'], ...]\n",
    "    returns:\n",
    "        tokenmap:  encoder dictionary\n",
    "        tokenmap_reverse: reversed tokenmap {index: word} to faciliate inverse lookup\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Write your code here\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenmap, tokenmap_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenmap, tokenmap_reverse = create_encoder(text, MIN_COUNT)\n",
    "VOCAB_SIZE = len(tokenmap)\n",
    "print(\"the reduced vocabulary size is:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T06:41:40.984760Z",
     "start_time": "2019-03-29T06:41:38.805679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoder the text using the encoder you just created\n",
    "def encode(text, tokenmap, default=0):\n",
    "    return [[tokenmap.get(t, default) for t in s] for s in text]\n",
    "\n",
    "text_encoded = encode(text, tokenmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct training context pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate training data, we need to find word-context pairs from the encoded text, \n",
    "we also want to generate some negative sample, so the input and output may look like:\n",
    "\n",
    "for input corpus: [[2, 3, 1, 2]] \n",
    "\n",
    "returns: [[word, context, label]]\n",
    "\n",
    "[[2, 3, 1], [2, 1, 1], [2, 2, 1], [3, 1, 1], ...., [4, 2, 0], [4, 3, 0], ...]\n",
    "\n",
    "Notice that in practice the sequence should be shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:44:15.373724Z",
     "start_time": "2019-03-30T01:44:15.230425Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "def training_data_generator(text_encoded, window_size=4, negative_samples=1.0, batch_docs=50):\n",
    "    \"\"\"\n",
    "    For given encoded text, return 3 np.array:\n",
    "    words, contexts, labels\n",
    "    Do not pair the w and its context cross different documents.\n",
    "    \n",
    "    input: \n",
    "        text_encoded: list of list of int, each list of int is the numerical encoding of the doc\n",
    "        window_size: int, define the context\n",
    "        negative_samples: float, how much negative sampling you need, normally 1.0\n",
    "        batch_docs: int, number of docs for which it generates one return\n",
    "        \n",
    "    yield:\n",
    "        words: list of int, the numerical encoding of the central words\n",
    "        contexts: list of int, the numerical encoding of the context words\n",
    "        labels: list of int, 1 or 0\n",
    "        \n",
    "    hint: \n",
    "    1. You can use skipgrams method from keras\n",
    "    2. For training purpose, words and contexts needs to be 2D array, with shape (N, 1), \n",
    "       but labels is 1D array, with shape (N, )\n",
    "    3. The output can be very big, you SHOULD using generator\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Write your code here\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a network that looks like this:\n",
    "<img src=\"skip-gram-NN.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:48:40.425395Z",
     "start_time": "2019-03-30T01:48:40.205518Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write your code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a simple version of training on batch code. You do not need to use\n",
    "opochs more than 10 since it will soon start shaking around the minimum. If you want \n",
    "to further improve your training, consider gradually increase the batch size or reduce \n",
    "the learning rate, then you can try for more than 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T00:50:28.355452Z",
     "start_time": "2019-03-30T00:50:28.251027Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "epochs = 10\n",
    "ntot = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d ======\" % epoch)\n",
    "    for words, contexts, labels in training_data_generator(text_encoded, batch_docs=50):\n",
    "        loss = model.train_on_batch(x=[words, contexts], y=labels)\n",
    "        ntot += len(words)\n",
    "        print(\"Total trained pairs (M): %10.2f ; \\t loss: %.4f\" % (ntot/1e6, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the embedding to a table\n",
    "\n",
    "Ready to translate the model you trained into the embedding DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T00:54:01.369150Z",
     "start_time": "2019-03-30T00:54:01.317001Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding2df(embedding_layer, tokenmap_reverse):\n",
    "    return (pd.DataFrame(embedding_layer.get_weights()[0], \n",
    "                        tokenmap_reverse.values())\n",
    "              .drop(\"_unknown_\", errors='ignore')\n",
    "              .transpose())\n",
    "\n",
    "skip = embedding2df(model.layers[2], tokenmap_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:10:10.760048Z",
     "start_time": "2019-03-30T01:10:08.741980Z"
    }
   },
   "source": [
    "Use the embedding you just trained, repeat the exploration you did for Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables:\n",
    "\n",
    "- pdf version of your final notebook\n",
    "- Discuss the questions in Section 3\n",
    "- If you have done any work to improve the model and model training, explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project (Not due this week)\n",
    "\n",
    "**Work with your teammates and start working on your final project proposal, think about these questions:**\n",
    "- The problem you try to solve and the value of this problem\n",
    "- Some current solution to this problem, reference citation if needed\n",
    "- Outline your approach and the goal you want to achieve\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
